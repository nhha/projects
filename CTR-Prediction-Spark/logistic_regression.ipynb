{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-44-147.us-west-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"uri\",\"mongodb://34.210.118.215/gameofspark.train\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- C1: integer (nullable = true)\n",
      " |-- C14: integer (nullable = true)\n",
      " |-- C15: integer (nullable = true)\n",
      " |-- C16: integer (nullable = true)\n",
      " |-- C17: integer (nullable = true)\n",
      " |-- C18: integer (nullable = true)\n",
      " |-- C19: integer (nullable = true)\n",
      " |-- C20: integer (nullable = true)\n",
      " |-- C21: integer (nullable = true)\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- app_category: string (nullable = true)\n",
      " |-- app_domain: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- banner_pos: integer (nullable = true)\n",
      " |-- click: integer (nullable = true)\n",
      " |-- device_conn_type: integer (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_ip: string (nullable = true)\n",
      " |-- device_model: string (nullable = true)\n",
      " |-- device_type: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- id: double (nullable = true)\n",
      " |-- site_category: string (nullable = true)\n",
      " |-- site_domain: string (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexStringCols(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "def oneHotEncodeCols(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract hour\n",
    "data = data.withColumn('hh', (f.col('hour')%100))\n",
    "# extract day of week\n",
    "data = data.withColumn('dow', (f.col('hour')/100-141019).cast('int')%7)\n",
    "# extract first 3 digits of ip address\n",
    "data = data.withColumn('ip', f.conv(data.device_ip.substr(0,2),16,10).alias('deci').cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringCols_noIds = ['app_category','site_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnumeric = indexStringCols(data, stringCols_noIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_selected = [\"C1\", \"banner_pos\", \"site_category\", \"app_category\", \"device_type\", \n",
    "                 \"device_conn_type\", \"C15\", \"C16\", \"C18\", \"C19\", \"C21\",\"ip\",\"hh\",\"dow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfonehot = oneHotEncodeCols(dfnumeric, feat_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=feat_selected, outputCol=\"features\")\n",
    "dfassembled = va.transform(dfonehot).select(\"features\", \"click\").withColumnRenamed(\"click\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dfassembled.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0].cache()\n",
    "df_test = splits[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_base = LogisticRegression()\n",
    "lr_basemodel = lr_base.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_basemodel_eval = lr_basemodel.evaluate(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='regParam', doc='regularization parameter (>= 0).'): 0.0,\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06,\n",
       " Param(parent=u'LogisticRegression_48e5b260f31a5188ea07', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_base.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions = lr_basemodel_eval.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy: 0.830603398635\n",
      "f1: 0.75916575695\n",
      "AUC: 0.683051308799\n"
     ]
    }
   ],
   "source": [
    "print 'accuracy:', evaluator.evaluate(base_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "print 'f1:', evaluator.evaluate(base_predictions, {evaluator.metricName: \"f1\"})\n",
    "print 'AUC:', lr_basemodel_eval.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-fold cross validation\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10000)\n",
    "bceval = BinaryClassificationEvaluator(metricName='areaUnderROC')\n",
    "cv = CrossValidator().setEstimator(lr).setEvaluator(bceval).setNumFolds(5)\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.elasticNetParam, [0.0, 0.01, 0.05, 0.1, 0.2, 0.5, 0.8, 1.0]).build()\n",
    "cv.setEstimatorParamMaps(paramGrid)\n",
    "cvmodel = cv.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print cvmodel.bestModel._java_obj.getElasticNetParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.683070801184\n"
     ]
    }
   ],
   "source": [
    "predictions = cvmodel.bestModel.transform(df_test)\n",
    "print bceval.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
